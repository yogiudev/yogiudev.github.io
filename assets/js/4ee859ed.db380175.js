"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7004],{2750:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"guides/Vector Database & Embeddings_ Complete Guide with","title":"Vector Database & Embeddings_ Complete Guide with","description":"What are Embeddings?","source":"@site/docs/guides/Vector Database & Embeddings_ Complete Guide with.md","sourceDirName":"guides","slug":"/guides/Vector Database & Embeddings_ Complete Guide with","permalink":"/docs/guides/Vector Database & Embeddings_ Complete Guide with","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/guides/Vector Database & Embeddings_ Complete Guide with.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Guides","permalink":"/docs/category/guides"},"next":{"title":"how_to_generate","permalink":"/docs/guides/how_to_generate"}}');var a=t(4848),r=t(8453);const s={},o="Vector Database & Embeddings: Complete Guide with Qdrant",c={},d=[{value:"What are Embeddings?",id:"what-are-embeddings",level:2},{value:"Why Vector Databases?",id:"why-vector-databases",level:3},{value:"Getting Started with Qdrant",id:"getting-started-with-qdrant",level:2},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Qdrant Setup",id:"basic-qdrant-setup",level:3},{value:"Understanding Embeddings in Detail",id:"understanding-embeddings-in-detail",level:2},{value:"How Embeddings Are Created",id:"how-embeddings-are-created",level:3},{value:"Embedding Similarity Visualization",id:"embedding-similarity-visualization",level:3},{value:"Core Vector Database Operations",id:"core-vector-database-operations",level:2},{value:"1. Inserting Data with Embeddings",id:"1-inserting-data-with-embeddings",level:3},{value:"2. Semantic Search and Matching",id:"2-semantic-search-and-matching",level:3},{value:"3. Advanced Filtering and Hybrid Search",id:"3-advanced-filtering-and-hybrid-search",level:3},{value:"Optimization Techniques",id:"optimization-techniques",level:2},{value:"1. Smart Embedding Caching",id:"1-smart-embedding-caching",level:3},{value:"2. Batch Processing for Efficiency",id:"2-batch-processing-for-efficiency",level:3},{value:"3. Embedding Quantization for Storage Efficiency",id:"3-embedding-quantization-for-storage-efficiency",level:3},{value:"Advanced Search Patterns",id:"advanced-search-patterns",level:2},{value:"1. Multi-Modal Search",id:"1-multi-modal-search",level:3},{value:"2. Similarity Clustering",id:"2-similarity-clustering",level:3},{value:"Tips and Tricks",id:"tips-and-tricks",level:2},{value:"1. <strong>Choose the Right Distance Metric</strong>",id:"1-choose-the-right-distance-metric",level:3},{value:"2. <strong>Optimize for Your Use Case</strong>",id:"2-optimize-for-your-use-case",level:3},{value:"3. <strong>Performance Monitoring</strong>",id:"3-performance-monitoring",level:3},{value:"4. <strong>Advanced Filtering Strategies</strong>",id:"4-advanced-filtering-strategies",level:3},{value:"Production Best Practices",id:"production-best-practices",level:2},{value:"1. <strong>Error Handling and Resilience</strong>",id:"1-error-handling-and-resilience",level:3},{value:"2. <strong>Backup and Recovery</strong>",id:"2-backup-and-recovery",level:3},{value:"3. <strong>Health Monitoring</strong>",id:"3-health-monitoring",level:3}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)("img",{src:"https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png",class:"logo",width:"120"}),"\n",(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"vector-database--embeddings-complete-guide-with-qdrant",children:"Vector Database & Embeddings: Complete Guide with Qdrant"})}),"\n",(0,a.jsx)(n.h2,{id:"what-are-embeddings",children:"What are Embeddings?"}),"\n",(0,a.jsxs)(n.p,{children:["Embeddings are ",(0,a.jsx)(n.strong,{children:"dense numerical representations"})," of data (text, images, audio) that capture semantic meaning in a high-dimensional space. Think of them as coordinates in a multi-dimensional space where similar items are positioned closer together."]}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Raw Text: \'Dog\'"] --\x3e B[Embedding Model]\n    C["Raw Text: \'Puppy\'"] --\x3e B\n    D["Raw Text: \'Car\'"] --\x3e B\n\n    B --\x3e E["Vector: [0.2, 0.8, 0.1, ...]"]\n    B --\x3e F["Vector: [0.3, 0.7, 0.2, ...]"]\n    B --\x3e G["Vector: [0.9, 0.1, 0.8, ...]"]\n\n    E --\x3e H["Similar vectors closer together"]\n    F --\x3e H\n    G --\x3e I["Dissimilar vectors farther apart"]'}),"\n",(0,a.jsx)(n.h3,{id:"why-vector-databases",children:"Why Vector Databases?"}),"\n",(0,a.jsxs)(n.p,{children:["Traditional databases store exact matches, while vector databases enable ",(0,a.jsx)(n.strong,{children:"semantic search"})," - finding items based on meaning rather than exact keywords."]}),"\n",(0,a.jsx)(n.h2,{id:"getting-started-with-qdrant",children:"Getting Started with Qdrant"}),"\n",(0,a.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Install Qdrant Python client\npip install qdrant-client sentence-transformers numpy\n\n# Run Qdrant server with Docker\ndocker run -p 6333:6333 qdrant/qdrant\n"})}),"\n",(0,a.jsx)(n.h3,{id:"basic-qdrant-setup",children:"Basic Qdrant Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport uuid\n\n# Initialize Qdrant client\nclient = QdrantClient(host="localhost", port=6333)\n\n# Initialize embedding model\nencoder = SentenceTransformer(\'all-MiniLM-L6-v2\')\n\n# Create collection\ncollection_name = "documents"\nclient.create_collection(\n    collection_name=collection_name,\n    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"understanding-embeddings-in-detail",children:"Understanding Embeddings in Detail"}),"\n",(0,a.jsx)(n.h3,{id:"how-embeddings-are-created",children:"How Embeddings Are Created"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_embeddings(texts):\n    """Create embeddings from text data"""\n    # Generate embeddings\n    embeddings = encoder.encode(texts)\n    \n    # Normalize embeddings (optional but recommended)\n    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    \n    return embeddings\n\n# Example usage\nsample_texts = [\n    "The quick brown fox jumps over the lazy dog",\n    "A fast brown fox leaps over a sleepy dog",\n    "Python is a programming language",\n    "Machine learning uses neural networks"\n]\n\nembeddings = create_embeddings(sample_texts)\nprint(f"Embedding shape: {embeddings.shape}")\nprint(f"First embedding: {embeddings[0][:5]}...")  # Show first 5 dimensions\n'})}),"\n",(0,a.jsx)(n.h3,{id:"embedding-similarity-visualization",children:"Embedding Similarity Visualization"}),"\n",(0,a.jsx)(n.mermaid,{value:"flowchart LR\n    A[Query: fast animal] --\x3e B[Embedding Model]\n    B --\x3e C[Query Vector]\n    \n    D[Database Vectors] --\x3e E[Similarity Calculation]\n    C --\x3e E\n    \n    E --\x3e F[Cosine Similarity Scores]\n    F --\x3e G[Ranked Results]\n    \n    G --\x3e H[1 quick brown fox - 0.89]\n    G --\x3e I[2 fast brown fox - 0.85]\n    G --\x3e J[3 programming language - 0.21]\n\n"}),"\n",(0,a.jsx)(n.h2,{id:"core-vector-database-operations",children:"Core Vector Database Operations"}),"\n",(0,a.jsx)(n.h3,{id:"1-inserting-data-with-embeddings",children:"1. Inserting Data with Embeddings"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def insert_documents(documents, collection_name):\n    """Insert documents with their embeddings"""\n    \n    # Create embeddings\n    embeddings = create_embeddings(documents)\n    \n    # Prepare points for insertion\n    points = []\n    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n        point = PointStruct(\n            id=str(uuid.uuid4()),\n            vector=embedding.tolist(),\n            payload={"text": doc, "index": i}\n        )\n        points.append(point)\n    \n    # Insert into Qdrant\n    client.upsert(\n        collection_name=collection_name,\n        points=points\n    )\n    \n    print(f"Inserted {len(documents)} documents")\n    return points\n\n# Insert sample documents\ndocuments = [\n    "Python is a versatile programming language",\n    "Machine learning enables pattern recognition",\n    "Neural networks mimic brain functions",\n    "Data science involves statistical analysis",\n    "Artificial intelligence automates decision making",\n    "Deep learning uses multiple neural layers",\n    "Natural language processing understands human language",\n    "Computer vision analyzes visual data"\n]\n\ninserted_points = insert_documents(documents, collection_name)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-semantic-search-and-matching",children:"2. Semantic Search and Matching"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def semantic_search(query, collection_name, top_k=3):\n    """Perform semantic search"""\n    \n    # Create query embedding\n    query_embedding = encoder.encode([query])\n    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n    \n    # Search in Qdrant\n    results = client.search(\n        collection_name=collection_name,\n        query_vector=query_embedding[0].tolist(),\n        limit=top_k,\n        score_threshold=0.3  # Minimum similarity threshold\n    )\n    \n    return results\n\n# Example searches\nqueries = [\n    "learning algorithms",\n    "brain-like computing",\n    "text understanding",\n    "visual recognition"\n]\n\nfor query in queries:\n    print(f"\\nQuery: \'{query}\'")\n    results = semantic_search(query, collection_name)\n    \n    for result in results:\n        print(f"  Score: {result.score:.3f} - {result.payload[\'text\']}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-advanced-filtering-and-hybrid-search",children:"3. Advanced Filtering and Hybrid Search"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def hybrid_search(query, collection_name, filters=None, top_k=3):\n    """Perform hybrid search with filters"""\n    \n    query_embedding = encoder.encode([query])\n    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n    \n    # Build filter conditions\n    filter_conditions = None\n    if filters:\n        from qdrant_client.models import Filter, FieldCondition, MatchValue\n        \n        conditions = []\n        for key, value in filters.items():\n            conditions.append(\n                FieldCondition(key=key, match=MatchValue(value=value))\n            )\n        \n        filter_conditions = Filter(must=conditions)\n    \n    # Search with filters\n    results = client.search(\n        collection_name=collection_name,\n        query_vector=query_embedding[0].tolist(),\n        query_filter=filter_conditions,\n        limit=top_k\n    )\n    \n    return results\n\n# Example with metadata filtering\n# First, let\'s add some metadata to our documents\ndef insert_documents_with_metadata(documents, metadata, collection_name):\n    """Insert documents with metadata"""\n    \n    embeddings = create_embeddings(documents)\n    \n    points = []\n    for i, (doc, embedding, meta) in enumerate(zip(documents, embeddings, metadata)):\n        point = PointStruct(\n            id=str(uuid.uuid4()),\n            vector=embedding.tolist(),\n            payload={"text": doc, "category": meta["category"], "difficulty": meta["difficulty"]}\n        )\n        points.append(point)\n    \n    client.upsert(collection_name=collection_name, points=points)\n    return points\n\n# Add metadata to documents\ndocument_metadata = [\n    {"category": "programming", "difficulty": "beginner"},\n    {"category": "ai", "difficulty": "intermediate"},\n    {"category": "ai", "difficulty": "advanced"},\n    {"category": "data", "difficulty": "intermediate"},\n    {"category": "ai", "difficulty": "advanced"},\n    {"category": "ai", "difficulty": "advanced"},\n    {"category": "ai", "difficulty": "intermediate"},\n    {"category": "ai", "difficulty": "intermediate"}\n]\n\n# Create new collection with metadata\ncollection_name_meta = "documents_with_metadata"\nclient.create_collection(\n    collection_name=collection_name_meta,\n    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n)\n\ninsert_documents_with_metadata(documents, document_metadata, collection_name_meta)\n\n# Search with filters\nresults = hybrid_search(\n    "machine learning", \n    collection_name_meta, \n    filters={"category": "ai", "difficulty": "intermediate"}\n)\n\nprint("\\nFiltered search results:")\nfor result in results:\n    print(f"Score: {result.score:.3f} - {result.payload[\'text\']}")\n    print(f"Category: {result.payload[\'category\']}, Difficulty: {result.payload[\'difficulty\']}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"optimization-techniques",children:"Optimization Techniques"}),"\n",(0,a.jsx)(n.h3,{id:"1-smart-embedding-caching",children:"1. Smart Embedding Caching"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import hashlib\nimport pickle\nfrom typing import Dict, List\n\nclass EmbeddingCache:\n    """Cache for storing computed embeddings"""\n    \n    def __init__(self, cache_file="embeddings_cache.pkl"):\n        self.cache_file = cache_file\n        self.cache = self._load_cache()\n    \n    def _load_cache(self) -> Dict:\n        """Load cache from file"""\n        try:\n            with open(self.cache_file, \'rb\') as f:\n                return pickle.load(f)\n        except FileNotFoundError:\n            return {}\n    \n    def _save_cache(self):\n        """Save cache to file"""\n        with open(self.cache_file, \'wb\') as f:\n            pickle.dump(self.cache, f)\n    \n    def get_embedding(self, text: str) -> np.ndarray:\n        """Get embedding from cache or compute new one"""\n        text_hash = hashlib.md5(text.encode()).hexdigest()\n        \n        if text_hash in self.cache:\n            return self.cache[text_hash]\n        \n        # Compute new embedding\n        embedding = encoder.encode([text])[0]\n        embedding = embedding / np.linalg.norm(embedding)\n        \n        # Cache the result\n        self.cache[text_hash] = embedding\n        self._save_cache()\n        \n        return embedding\n\n# Initialize cache\nembedding_cache = EmbeddingCache()\n\ndef cached_search(query, collection_name, top_k=3):\n    """Search using cached embeddings"""\n    \n    # Get cached embedding\n    query_embedding = embedding_cache.get_embedding(query)\n    \n    results = client.search(\n        collection_name=collection_name,\n        query_vector=query_embedding.tolist(),\n        limit=top_k\n    )\n    \n    return results\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-batch-processing-for-efficiency",children:"2. Batch Processing for Efficiency"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def batch_insert_documents(documents, collection_name, batch_size=100):\n    """Insert documents in batches for better performance"""\n    \n    for i in range(0, len(documents), batch_size):\n        batch = documents[i:i + batch_size]\n        \n        # Create embeddings for batch\n        embeddings = create_embeddings(batch)\n        \n        # Prepare points\n        points = []\n        for j, (doc, embedding) in enumerate(zip(batch, embeddings)):\n            point = PointStruct(\n                id=str(uuid.uuid4()),\n                vector=embedding.tolist(),\n                payload={"text": doc, "batch_index": i + j}\n            )\n            points.append(point)\n        \n        # Insert batch\n        client.upsert(collection_name=collection_name, points=points)\n        print(f"Inserted batch {i//batch_size + 1}: {len(batch)} documents")\n\n# Example: Insert 1000 documents in batches\nlarge_document_set = [f"Document {i} about various topics" for i in range(1000)]\nbatch_insert_documents(large_document_set, collection_name)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-embedding-quantization-for-storage-efficiency",children:"3. Embedding Quantization for Storage Efficiency"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def quantize_embeddings(embeddings, num_bits=8):\n    """Quantize embeddings to reduce storage"""\n    \n    # Find min/max values\n    min_val = np.min(embeddings)\n    max_val = np.max(embeddings)\n    \n    # Quantize\n    scale = (max_val - min_val) / (2**num_bits - 1)\n    quantized = np.round((embeddings - min_val) / scale).astype(np.uint8)\n    \n    return quantized, min_val, scale\n\ndef dequantize_embeddings(quantized, min_val, scale):\n    """Dequantize embeddings"""\n    return quantized.astype(np.float32) * scale + min_val\n\n# Example usage\noriginal_embeddings = create_embeddings(["Sample text for quantization"])\nquantized, min_val, scale = quantize_embeddings(original_embeddings)\nrestored = dequantize_embeddings(quantized, min_val, scale)\n\nprint(f"Original size: {original_embeddings.nbytes} bytes")\nprint(f"Quantized size: {quantized.nbytes} bytes")\nprint(f"Compression ratio: {original_embeddings.nbytes / quantized.nbytes:.2f}x")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-search-patterns",children:"Advanced Search Patterns"}),"\n",(0,a.jsx)(n.h3,{id:"1-multi-modal-search",children:"1. Multi-Modal Search"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def multi_modal_search(query, collection_name, search_types=["semantic", "keyword"]):\n    """Combine multiple search approaches"""\n    \n    results = {}\n    \n    if "semantic" in search_types:\n        # Semantic search\n        semantic_results = semantic_search(query, collection_name, top_k=10)\n        results["semantic"] = semantic_results\n    \n    if "keyword" in search_types:\n        # Keyword-based search using payload filtering\n        from qdrant_client.models import Filter, FieldCondition, MatchText\n        \n        keyword_results = client.search(\n            collection_name=collection_name,\n            query_vector=[0] * 384,  # Dummy vector\n            query_filter=Filter(\n                must=[FieldCondition(key="text", match=MatchText(text=query))]\n            ),\n            limit=10\n        )\n        results["keyword"] = keyword_results\n    \n    return results\n\n# Example multi-modal search\nmulti_results = multi_modal_search("neural networks", collection_name)\nprint("Semantic results:", len(multi_results["semantic"]))\nprint("Keyword results:", len(multi_results["keyword"]))\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-similarity-clustering",children:"2. Similarity Clustering"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def find_similar_clusters(collection_name, similarity_threshold=0.8):\n    """Find clusters of similar documents"""\n    \n    # Get all points\n    all_points = client.scroll(collection_name=collection_name, limit=1000)[0]\n    \n    clusters = []\n    processed = set()\n    \n    for point in all_points:\n        if point.id in processed:\n            continue\n        \n        # Find similar documents\n        similar_docs = client.search(\n            collection_name=collection_name,\n            query_vector=point.vector,\n            limit=10,\n            score_threshold=similarity_threshold\n        )\n        \n        cluster = []\n        for doc in similar_docs:\n            if doc.id not in processed:\n                cluster.append(doc)\n                processed.add(doc.id)\n        \n        if len(cluster) > 1:\n            clusters.append(cluster)\n    \n    return clusters\n\n# Find document clusters\nclusters = find_similar_clusters(collection_name)\nprint(f"Found {len(clusters)} clusters")\n\nfor i, cluster in enumerate(clusters):\n    print(f"\\nCluster {i+1}:")\n    for doc in cluster:\n        print(f"  - {doc.payload[\'text\'][:50]}...")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"tips-and-tricks",children:"Tips and Tricks"}),"\n",(0,a.jsxs)(n.h3,{id:"1-choose-the-right-distance-metric",children:["1. ",(0,a.jsx)(n.strong,{children:"Choose the Right Distance Metric"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Distance metrics comparison\ndistance_metrics = {\n    "COSINE": "Best for normalized embeddings, measures angle",\n    "EUCLIDEAN": "Measures straight-line distance, sensitive to magnitude",\n    "DOT": "Fast but requires careful normalization"\n}\n\ndef benchmark_distance_metrics(query, documents):\n    """Compare different distance metrics"""\n    \n    for metric_name, description in distance_metrics.items():\n        print(f"\\n{metric_name}: {description}")\n        \n        # Create collection with specific metric\n        test_collection = f"test_{metric_name.lower()}"\n        \n        try:\n            client.delete_collection(test_collection)\n        except:\n            pass\n        \n        client.create_collection(\n            collection_name=test_collection,\n            vectors_config=VectorParams(\n                size=384, \n                distance=getattr(Distance, metric_name)\n            )\n        )\n        \n        # Insert and search\n        insert_documents(documents, test_collection)\n        results = semantic_search(query, test_collection, top_k=3)\n        \n        print(f"Top result score: {results[0].score:.3f}")\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"2-optimize-for-your-use-case",children:["2. ",(0,a.jsx)(n.strong,{children:"Optimize for Your Use Case"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def optimize_collection_config(expected_size, query_patterns):\n    """Optimize collection configuration"""\n    \n    config = {\n        "vectors_config": VectorParams(size=384, distance=Distance.COSINE),\n        "optimizers_config": {\n            "deleted_threshold": 0.2,\n            "vacuum_min_vector_number": 1000,\n            "default_segment_number": 0,\n            "max_segment_size": None,\n            "memmap_threshold": None,\n            "indexing_threshold": 20000,\n            "flush_interval_sec": 5,\n            "max_optimization_threads": 1\n        }\n    }\n    \n    # Adjust based on expected size\n    if expected_size > 100000:\n        config["optimizers_config"]["indexing_threshold"] = 10000\n        config["optimizers_config"]["max_optimization_threads"] = 4\n    \n    # Adjust based on query patterns\n    if "frequent_updates" in query_patterns:\n        config["optimizers_config"]["flush_interval_sec"] = 1\n    \n    return config\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"3-performance-monitoring",children:["3. ",(0,a.jsx)(n.strong,{children:"Performance Monitoring"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef performance_monitor(operation_name):\n    """Monitor operation performance"""\n    start_time = time.time()\n    try:\n        yield\n    finally:\n        end_time = time.time()\n        print(f"{operation_name} took {end_time - start_time:.3f} seconds")\n\n# Usage example\nwith performance_monitor("Semantic Search"):\n    results = semantic_search("machine learning", collection_name)\n\n# Memory usage monitoring\nimport psutil\nimport os\n\ndef monitor_memory_usage():\n    """Monitor memory usage"""\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    print(f"Memory usage: {memory_info.rss / 1024 / 1024:.1f} MB")\n\nmonitor_memory_usage()\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"4-advanced-filtering-strategies",children:["4. ",(0,a.jsx)(n.strong,{children:"Advanced Filtering Strategies"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_advanced_filters(conditions):\n    """Create complex filter conditions"""\n    from qdrant_client.models import Filter, FieldCondition, HasIdCondition, Range\n    \n    must_conditions = []\n    should_conditions = []\n    \n    for condition in conditions:\n        if condition["type"] == "exact_match":\n            must_conditions.append(\n                FieldCondition(key=condition["field"], match=condition["value"])\n            )\n        elif condition["type"] == "range":\n            must_conditions.append(\n                FieldCondition(\n                    key=condition["field"],\n                    range=Range(\n                        gte=condition.get("min"),\n                        lte=condition.get("max")\n                    )\n                )\n            )\n        elif condition["type"] == "id_list":\n            must_conditions.append(\n                HasIdCondition(has_id=condition["ids"])\n            )\n    \n    return Filter(must=must_conditions, should=should_conditions)\n\n# Example advanced filtering\nadvanced_conditions = [\n    {"type": "exact_match", "field": "category", "value": "ai"},\n    {"type": "range", "field": "difficulty_score", "min": 0.3, "max": 0.8}\n]\n\nfilter_condition = create_advanced_filters(advanced_conditions)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"production-best-practices",children:"Production Best Practices"}),"\n",(0,a.jsxs)(n.h3,{id:"1-error-handling-and-resilience",children:["1. ",(0,a.jsx)(n.strong,{children:"Error Handling and Resilience"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def resilient_search(query, collection_name, retries=3, timeout=30):\n    """Perform search with error handling"""\n    \n    for attempt in range(retries):\n        try:\n            # Set timeout\n            client.timeout = timeout\n            \n            results = semantic_search(query, collection_name)\n            return results\n            \n        except Exception as e:\n            print(f"Attempt {attempt + 1} failed: {str(e)}")\n            if attempt == retries - 1:\n                # Return empty results on final failure\n                return []\n            time.sleep(2 ** attempt)  # Exponential backoff\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"2-backup-and-recovery",children:["2. ",(0,a.jsx)(n.strong,{children:"Backup and Recovery"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def backup_collection(collection_name, backup_file):\n    """Backup collection data"""\n    \n    # Get all points\n    all_points, _ = client.scroll(collection_name=collection_name, limit=10000)\n    \n    # Save to file\n    backup_data = {\n        "collection_name": collection_name,\n        "points": [\n            {\n                "id": point.id,\n                "vector": point.vector,\n                "payload": point.payload\n            }\n            for point in all_points\n        ]\n    }\n    \n    with open(backup_file, \'w\') as f:\n        json.dump(backup_data, f)\n    \n    print(f"Backed up {len(all_points)} points to {backup_file}")\n\ndef restore_collection(backup_file):\n    """Restore collection from backup"""\n    \n    with open(backup_file, \'r\') as f:\n        backup_data = json.load(f)\n    \n    collection_name = backup_data["collection_name"]\n    \n    # Recreate collection\n    client.create_collection(\n        collection_name=collection_name,\n        vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n    )\n    \n    # Restore points\n    points = [\n        PointStruct(\n            id=point["id"],\n            vector=point["vector"],\n            payload=point["payload"]\n        )\n        for point in backup_data["points"]\n    ]\n    \n    client.upsert(collection_name=collection_name, points=points)\n    print(f"Restored {len(points)} points")\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"3-health-monitoring",children:["3. ",(0,a.jsx)(n.strong,{children:"Health Monitoring"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def health_check():\n    """Perform system health check"""\n    \n    try:\n        # Check Qdrant connection\n        info = client.get_collections()\n        print(f"\u2713 Qdrant connected - {len(info.collections)} collections")\n        \n        # Check embedding model\n        test_embedding = encoder.encode(["test"])\n        print(f"\u2713 Embedding model working - dimension: {len(test_embedding[0])}")\n        \n        # Check collection status\n        for collection in info.collections:\n            collection_info = client.get_collection(collection.name)\n            print(f"\u2713 Collection {collection.name}: {collection_info.points_count} points")\n        \n        return True\n        \n    except Exception as e:\n        print(f"\u2717 Health check failed: {str(e)}")\n        return False\n\n# Run health check\nhealth_check()\n'})}),"\n",(0,a.jsx)(n.p,{children:"This comprehensive guide covers everything you need to know about using vector databases with embeddings effectively. Start with the basics, experiment with the code examples, and gradually implement the optimization techniques based on your specific use case requirements."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);