"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7404],{3080:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"guides/langgraph","title":"LangGraph","description":"What is LangGraph?","source":"@site/docs/guides/langgraph.md","sourceDirName":"guides","slug":"/guides/langgraph","permalink":"/docs/guides/langgraph","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/guides/langgraph.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LangChain Guide","permalink":"/docs/guides/langchain"},"next":{"title":"LangGraph: Advanced","permalink":"/docs/guides/langraph-advanced"}}');var r=t(4848),i=t(8453);const a={},o="LangGraph",c={},l=[{value:"What is LangGraph?",id:"what-is-langgraph",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Graph Structure",id:"graph-structure",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Installation",id:"installation",level:3},{value:"Basic Graph Setup",id:"basic-graph-setup",level:3},{value:"Real-World Use Case: E-commerce Customer Service Agent",id:"real-world-use-case-e-commerce-customer-service-agent",level:2},{value:"Use Case Overview",id:"use-case-overview",level:3},{value:"Step 1: Define the State Schema",id:"step-1-define-the-state-schema",level:3},{value:"Step 2: Build the Agent Functions",id:"step-2-build-the-agent-functions",level:3},{value:"Step 3: Define Routing Logic",id:"step-3-define-routing-logic",level:3},{value:"Step 4: Build the Complete Graph",id:"step-4-build-the-complete-graph",level:3},{value:"Step 5: Test the System with Real Examples",id:"step-5-test-the-system-with-real-examples",level:3},{value:"Expected Outputs",id:"expected-outputs",level:3},{value:"Pro Tips and Tricks",id:"pro-tips-and-tricks",level:2},{value:"1. <strong>State Management Best Practices</strong>",id:"1-state-management-best-practices",level:3},{value:"2. <strong>Performance Optimization Tricks</strong>",id:"2-performance-optimization-tricks",level:3},{value:"3. <strong>Error Handling Patterns</strong>",id:"3-error-handling-patterns",level:3},{value:"4. <strong>Advanced Routing Strategies</strong>",id:"4-advanced-routing-strategies",level:3},{value:"5. <strong>Testing and Debugging Strategies</strong>",id:"5-testing-and-debugging-strategies",level:3},{value:"6. <strong>Memory and Persistence Tricks</strong>",id:"6-memory-and-persistence-tricks",level:3},{value:"7. <strong>Production Deployment Best Practices</strong>",id:"7-production-deployment-best-practices",level:3},{value:"Advanced Features in Action",id:"advanced-features-in-action",level:2},{value:"Streaming Results for Real-time Updates",id:"streaming-results-for-real-time-updates",level:3},{value:"Dynamic Graph Modification",id:"dynamic-graph-modification",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"langgraph",children:"LangGraph"})}),"\n",(0,r.jsx)(n.h1,{id:"complete-guide-to-building-multi-agent-workflows",children:"Complete Guide to Building Multi-Agent Workflows"}),"\n",(0,r.jsx)(n.h2,{id:"what-is-langgraph",children:"What is LangGraph?"}),"\n",(0,r.jsxs)(n.p,{children:["LangGraph is a powerful library within the LangChain ecosystem that enables you to create stateful, multi-agent applications using large language models. Unlike traditional linear chains, LangGraph allows you to build ",(0,r.jsx)(n.strong,{children:"cyclical graphs"})," where agents can interact, make decisions, and coordinate their actions in sophisticated workflows."]}),"\n",(0,r.jsx)(n.p,{children:"Think of LangGraph as a framework for orchestrating multiple AI agents in a structured, graph-based architecture where each node represents a unit of work and edges define the flow of information and control."}),"\n",(0,r.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,r.jsx)(n.h3,{id:"graph-structure",children:"Graph Structure"}),"\n",(0,r.jsxs)(n.p,{children:["At its heart, LangGraph uses a ",(0,r.jsx)(n.strong,{children:"directed graph"})," approach where:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Nodes"})," represent individual LLM agents or functions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Edges"})," serve as communication channels between agents"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State"})," maintains context and information across interactions"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Nodes"}),": Units of work within your graph, typically Python functions that perform specific tasks such as:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Interacting with an LLM"}),"\n",(0,r.jsx)(n.li,{children:"Calling external tools or APIs"}),"\n",(0,r.jsx)(n.li,{children:"Processing data"}),"\n",(0,r.jsx)(n.li,{children:"Handling user input"}),"\n",(0,r.jsx)(n.li,{children:"Executing business logic"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Edges"}),": Define the flow of information and execution order between nodes. LangGraph supports several types:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Normal Edges"}),": Direct, unconditional flow from one node to another"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Conditional Edges"}),": Dynamic branching based on conditions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Entry Points"}),": Define which node executes first"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Conditional Entry Points"}),": Use logic to determine the starting node"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"State"}),": A central object that gets updated over time by nodes in the graph, maintaining:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Conversation history"}),"\n",(0,r.jsx)(n.li,{children:"Contextual data"}),"\n",(0,r.jsx)(n.li,{children:"Internal variables and flags"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install -U langgraph langchain-openai\n"})}),"\n",(0,r.jsx)(n.h3,{id:"basic-graph-setup",children:"Basic Graph Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\n\n# Define the state structure\nclass State(TypedDict):\n    # Messages have the type "list"\n    # The add_messages function appends messages to the list\n    messages: Annotated[list, add_messages]\n\n# Create the graph builder\ngraph_builder = StateGraph(State)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"real-world-use-case-e-commerce-customer-service-agent",children:"Real-World Use Case: E-commerce Customer Service Agent"}),"\n",(0,r.jsx)(n.p,{children:"Let's build a comprehensive customer service system that handles multiple scenarios like order tracking, product recommendations, and billing inquiries. This system will showcase all LangGraph features in action."}),"\n",(0,r.jsx)(n.h3,{id:"use-case-overview",children:"Use Case Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scenario"}),": TechMart, an online electronics retailer, needs an AI customer service system that can:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Handle order inquiries and tracking"}),"\n",(0,r.jsx)(n.li,{children:"Provide product recommendations"}),"\n",(0,r.jsx)(n.li,{children:"Process returns and refunds"}),"\n",(0,r.jsx)(n.li,{children:"Escalate complex issues to human agents"}),"\n",(0,r.jsx)(n.li,{children:"Generate personalized follow-up emails"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-define-the-state-schema",children:"Step 1: Define the State Schema"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from typing import TypedDict, Annotated, List, Optional, Dict\nfrom langgraph.graph.message import add_messages\nfrom datetime import datetime\n\nclass CustomerServiceState(TypedDict):\n    # Core conversation\n    messages: Annotated[List, add_messages]\n    \n    # Customer information\n    customer_id: Optional[str]\n    customer_name: Optional[str]\n    customer_email: Optional[str]\n    \n    # Session tracking\n    intent: Optional[str]\n    session_id: str\n    conversation_stage: str\n    \n    # Business data\n    order_details: Optional[Dict]\n    product_recommendations: List[Dict]\n    support_ticket: Optional[Dict]\n    \n    # Control flow\n    needs_human_escalation: bool\n    retry_count: int\n    satisfaction_score: Optional[int]\n    \n    # Performance tracking\n    response_time: float\n    tools_used: List[str]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-build-the-agent-functions",children:"Step 2: Build the Agent Functions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, END, START\nimport json\nimport time\nfrom datetime import datetime\n\n# Initialize LLM\nllm = ChatOpenAI(model="gpt-4", temperature=0.7)\n\ndef intent_classifier(state: CustomerServiceState):\n    """First agent: Classify customer intent"""\n    start_time = time.time()\n    \n    user_message = state["messages"][-1].content\n    \n    classification_prompt = f"""\n    Analyze this customer message and classify the intent:\n    \n    Message: "{user_message}"\n    \n    Possible intents:\n    - order_inquiry: Questions about existing orders\n    - product_search: Looking for products or recommendations\n    - billing_issue: Payment or billing problems\n    - return_refund: Returns, exchanges, or refunds\n    - technical_support: Product issues or technical help\n    - general_inquiry: General questions or greetings\n    \n    Respond with just the intent name.\n    """\n    \n    response = llm.invoke([{"role": "user", "content": classification_prompt}])\n    intent = response.content.strip().lower()\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        "intent": intent,\n        "conversation_stage": "intent_classified",\n        "response_time": processing_time,\n        "tools_used": state["tools_used"] + ["intent_classifier"]\n    }\n\ndef order_tracking_agent(state: CustomerServiceState):\n    """Handle order-related inquiries"""\n    start_time = time.time()\n    \n    # Mock order lookup - in real app, this would query a database\n    def lookup_order(customer_id: str, order_reference: str = None):\n        # Simulated order data\n        return {\n            "order_id": "ORD-2024-001234",\n            "status": "shipped",\n            "tracking_number": "1Z999AA1234567890",\n            "estimated_delivery": "2024-01-20",\n            "items": [\n                {"name": "iPhone 15 Pro", "quantity": 1, "price": 999.99},\n                {"name": "MagSafe Charger", "quantity": 1, "price": 39.99}\n            ],\n            "total": 1039.98\n        }\n    \n    # Extract order info from conversation\n    user_message = state["messages"][-1].content\n    order_details = lookup_order(state.get("customer_id", "CUST001"))\n    \n    response_prompt = f"""\n    You\'re a helpful customer service agent. The customer asked: "{user_message}"\n    \n    Here\'s their order information:\n    {json.dumps(order_details, indent=2)}\n    \n    Provide a helpful, friendly response about their order status.\n    Include tracking information and estimated delivery date.\n    """\n    \n    response = llm.invoke([{"role": "user", "content": response_prompt}])\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        "messages": [("assistant", response.content)],\n        "order_details": order_details,\n        "conversation_stage": "order_resolved",\n        "response_time": processing_time,\n        "tools_used": state["tools_used"] + ["order_tracking"]\n    }\n\ndef product_recommendation_agent(state: CustomerServiceState):\n    """Provide product recommendations"""\n    start_time = time.time()\n    \n    # Mock product database\n    products = [\n        {"id": "P001", "name": "iPhone 15 Pro Max", "price": 1199.99, "category": "smartphones", "rating": 4.8},\n        {"id": "P002", "name": "Samsung Galaxy S24", "price": 899.99, "category": "smartphones", "rating": 4.7},\n        {"id": "P003", "name": "MacBook Pro 14", "price": 1999.99, "category": "laptops", "rating": 4.9},\n        {"id": "P004", "name": "iPad Air", "price": 599.99, "category": "tablets", "rating": 4.6},\n        {"id": "P005", "name": "AirPods Pro", "price": 249.99, "category": "audio", "rating": 4.5}\n    ]\n    \n    user_message = state["messages"][-1].content\n    \n    # Simple recommendation logic (in real app, use ML/embeddings)\n    recommendations = []\n    if "phone" in user_message.lower() or "smartphone" in user_message.lower():\n        recommendations = [p for p in products if p["category"] == "smartphones"]\n    elif "laptop" in user_message.lower() or "computer" in user_message.lower():\n        recommendations = [p for p in products if p["category"] == "laptops"]\n    else:\n        recommendations = products[:3]  # Top 3 products\n    \n    rec_text = "\\n".join([\n        f"\u2022 {p[\'name\']} - ${p[\'price\']} (Rating: {p[\'rating\']}/5)"\n        for p in recommendations\n    ])\n    \n    response_prompt = f"""\n    The customer asked: "{user_message}"\n    \n    Here are some great product recommendations:\n    {rec_text}\n    \n    Provide a helpful response with these recommendations, highlighting key features and benefits.\n    Ask if they\'d like more details about any specific product.\n    """\n    \n    response = llm.invoke([{"role": "user", "content": response_prompt}])\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        "messages": [("assistant", response.content)],\n        "product_recommendations": recommendations,\n        "conversation_stage": "recommendations_provided",\n        "response_time": processing_time,\n        "tools_used": state["tools_used"] + ["product_recommendations"]\n    }\n\ndef billing_support_agent(state: CustomerServiceState):\n    """Handle billing and payment issues"""\n    start_time = time.time()\n    \n    # Mock billing data\n    billing_info = {\n        "account_balance": 0.00,\n        "last_payment": "2024-01-15",\n        "last_payment_amount": 1039.98,\n        "payment_method": "Credit Card (**** 1234)",\n        "billing_address": "123 Main St, Anytown, USA 12345"\n    }\n    \n    user_message = state["messages"][-1].content\n    \n    # Check if this needs escalation\n    escalation_keywords = ["fraud", "dispute", "unauthorized", "cancel account", "legal"]\n    needs_escalation = any(keyword in user_message.lower() for keyword in escalation_keywords)\n    \n    if needs_escalation:\n        response_content = """\n        I understand this is a serious concern that requires immediate attention. \n        I\'m escalating your case to our billing specialist who will contact you within 2 hours.\n        \n        Your case number is: BIL-2024-5678\n        \n        In the meantime, if this involves unauthorized charges, please contact your bank immediately.\n        """\n        \n        support_ticket = {\n            "ticket_id": "BIL-2024-5678",\n            "type": "billing_escalation",\n            "priority": "high",\n            "created_at": datetime.now().isoformat(),\n            "description": user_message\n        }\n        \n        stage = "escalated_to_human"\n        escalation_flag = True\n    else:\n        response_prompt = f"""\n        You\'re a billing support agent. The customer asked: "{user_message}"\n        \n        Here\'s their billing information:\n        {json.dumps(billing_info, indent=2)}\n        \n        Provide a helpful response about their billing inquiry.\n        Be professional and address their concerns clearly.\n        """\n        \n        response = llm.invoke([{"role": "user", "content": response_prompt}])\n        response_content = response.content\n        support_ticket = None\n        stage = "billing_resolved"\n        escalation_flag = False\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        "messages": [("assistant", response_content)],\n        "support_ticket": support_ticket,\n        "conversation_stage": stage,\n        "needs_human_escalation": escalation_flag,\n        "response_time": processing_time,\n        "tools_used": state["tools_used"] + ["billing_support"]\n    }\n\ndef satisfaction_survey_agent(state: CustomerServiceState):\n    """Collect customer satisfaction feedback"""\n    start_time = time.time()\n    \n    survey_message = """\n    Thank you for contacting TechMart support! Before we end this conversation, \n    could you please rate your experience today on a scale of 1-5?\n    \n    1 = Very Dissatisfied\n    2 = Dissatisfied  \n    3 = Neutral\n    4 = Satisfied\n    5 = Very Satisfied\n    \n    Your feedback helps us improve our service!\n    """\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        "messages": [("assistant", survey_message)],\n        "conversation_stage": "satisfaction_survey",\n        "response_time": processing_time,\n        "tools_used": state["tools_used"] + ["satisfaction_survey"]\n    }\n\ndef human_escalation_agent(state: CustomerServiceState):\n    """Handle escalation to human agents"""\n    start_time = time.time()\n    \n    escalation_message = f"""\n    I\'ve created a priority support ticket for you: {state[\'support_ticket\'][\'ticket_id\']}\n    \n    A human specialist will review your case and contact you within 2 hours at {state.get(\'customer_email\', \'your registered email\')}.\n    \n    Is there anything else I can help you with while you wait?\n    """\n    \n    processing_time = time.time() - start_time\n    \n    return {\n        "messages": [("assistant", escalation_message)],\n        "conversation_stage": "escalated_complete",\n        "response_time": processing_time,\n        "tools_used": state["tools_used"] + ["human_escalation"]\n    }\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-define-routing-logic",children:"Step 3: Define Routing Logic"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def route_by_intent(state: CustomerServiceState):\n    """Route customer to appropriate agent based on intent"""\n    intent = state.get("intent", "general_inquiry")\n    \n    routing_map = {\n        "order_inquiry": "order_tracking",\n        "product_search": "product_recommendations", \n        "billing_issue": "billing_support",\n        "return_refund": "billing_support",  # Use billing agent for refunds\n        "technical_support": "human_escalation",  # Tech issues go to humans\n        "general_inquiry": "product_recommendations"  # Default to recommendations\n    }\n    \n    return routing_map.get(intent, "product_recommendations")\n\ndef check_escalation_needed(state: CustomerServiceState):\n    """Check if conversation needs human escalation"""\n    if state.get("needs_human_escalation", False):\n        return "human_escalation"\n    elif state.get("conversation_stage") in ["order_resolved", "recommendations_provided", "billing_resolved"]:\n        return "satisfaction_survey"\n    else:\n        return "continue_conversation"\n\ndef handle_satisfaction_response(state: CustomerServiceState):\n    """Process satisfaction survey response"""\n    user_message = state["messages"][-1].content\n    \n    # Extract rating (simple approach)\n    rating = None\n    for char in user_message:\n        if char.isdigit():\n            rating = int(char)\n            break\n    \n    if rating:\n        if rating >= 4:\n            return "end_positive"\n        elif rating <= 2:\n            return "escalate_feedback"\n        else:\n            return "end_neutral"\n    else:\n        return "end_neutral"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-build-the-complete-graph",children:"Step 4: Build the Complete Graph"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Initialize the graph\nworkflow = StateGraph(CustomerServiceState)\n\n# Add all agent nodes\nworkflow.add_node("intent_classifier", intent_classifier)\nworkflow.add_node("order_tracking", order_tracking_agent)\nworkflow.add_node("product_recommendations", product_recommendation_agent)\nworkflow.add_node("billing_support", billing_support_agent)\nworkflow.add_node("satisfaction_survey", satisfaction_survey_agent)\nworkflow.add_node("human_escalation", human_escalation_agent)\n\n# Set entry point\nworkflow.set_entry_point("intent_classifier")\n\n# Add routing from intent classifier\nworkflow.add_conditional_edges(\n    "intent_classifier",\n    route_by_intent,\n    {\n        "order_tracking": "order_tracking",\n        "product_recommendations": "product_recommendations",\n        "billing_support": "billing_support",\n        "human_escalation": "human_escalation"\n    }\n)\n\n# Add escalation checks for each agent\nworkflow.add_conditional_edges(\n    "order_tracking",\n    check_escalation_needed,\n    {\n        "satisfaction_survey": "satisfaction_survey",\n        "human_escalation": "human_escalation"\n    }\n)\n\nworkflow.add_conditional_edges(\n    "product_recommendations", \n    check_escalation_needed,\n    {\n        "satisfaction_survey": "satisfaction_survey",\n        "human_escalation": "human_escalation"\n    }\n)\n\nworkflow.add_conditional_edges(\n    "billing_support",\n    check_escalation_needed,\n    {\n        "satisfaction_survey": "satisfaction_survey", \n        "human_escalation": "human_escalation"\n    }\n)\n\n# Handle satisfaction survey responses\nworkflow.add_conditional_edges(\n    "satisfaction_survey",\n    handle_satisfaction_response,\n    {\n        "end_positive": END,\n        "end_neutral": END,\n        "escalate_feedback": "human_escalation"\n    }\n)\n\n# Human escalation goes to end\nworkflow.add_edge("human_escalation", END)\n\n# Compile the graph\napp = workflow.compile()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-5-test-the-system-with-real-examples",children:"Step 5: Test the System with Real Examples"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Test Case 1: Order Inquiry\ndef test_order_inquiry():\n    """Test order tracking functionality"""\n    initial_state = {\n        "messages": [("user", "Hi, I\'d like to check the status of my recent order")],\n        "customer_id": "CUST001",\n        "customer_name": "John Doe",\n        "customer_email": "john@example.com",\n        "session_id": "session_001",\n        "conversation_stage": "initial",\n        "needs_human_escalation": False,\n        "retry_count": 0,\n        "response_time": 0.0,\n        "tools_used": []\n    }\n    \n    result = app.invoke(initial_state)\n    return result\n\n# Test Case 2: Product Search\ndef test_product_search():\n    """Test product recommendation functionality"""\n    initial_state = {\n        "messages": [("user", "I\'m looking for a new smartphone under $1000")],\n        "customer_id": "CUST002", \n        "customer_name": "Jane Smith",\n        "customer_email": "jane@example.com",\n        "session_id": "session_002",\n        "conversation_stage": "initial",\n        "needs_human_escalation": False,\n        "retry_count": 0,\n        "response_time": 0.0,\n        "tools_used": []\n    }\n    \n    result = app.invoke(initial_state)\n    return result\n\n# Test Case 3: Billing Issue (Escalation)\ndef test_billing_escalation():\n    """Test billing issue that requires escalation"""\n    initial_state = {\n        "messages": [("user", "I see an unauthorized charge on my account for $500. This looks like fraud!")],\n        "customer_id": "CUST003",\n        "customer_name": "Bob Johnson", \n        "customer_email": "bob@example.com",\n        "session_id": "session_003",\n        "conversation_stage": "initial",\n        "needs_human_escalation": False,\n        "retry_count": 0,\n        "response_time": 0.0,\n        "tools_used": []\n    }\n    \n    result = app.invoke(initial_state)\n    return result\n\n# Run tests and show results\nprint("=== Test 1: Order Inquiry ===")\norder_result = test_order_inquiry()\nprint(f"Final stage: {order_result[\'conversation_stage\']}")\nprint(f"Tools used: {order_result[\'tools_used\']}")\nprint(f"Response: {order_result[\'messages\'][-1].content}")\n\nprint("\\n=== Test 2: Product Search ===")\nproduct_result = test_product_search()\nprint(f"Final stage: {product_result[\'conversation_stage\']}")\nprint(f"Recommendations: {len(product_result.get(\'product_recommendations\', []))} products")\nprint(f"Response: {product_result[\'messages\'][-1].content}")\n\nprint("\\n=== Test 3: Billing Escalation ===")\nbilling_result = test_billing_escalation()\nprint(f"Final stage: {billing_result[\'conversation_stage\']}")\nprint(f"Escalated: {billing_result[\'needs_human_escalation\']}")\nprint(f"Ticket ID: {billing_result.get(\'support_ticket\', {}).get(\'ticket_id\')}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"expected-outputs",children:"Expected Outputs"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test 1 - Order Inquiry Expected Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Final stage: satisfaction_survey\nTools used: ['intent_classifier', 'order_tracking', 'satisfaction_survey']\nResponse: Thank you for contacting TechMart support! Before we end this conversation, \ncould you please rate your experience today on a scale of 1-5?\n\n1 = Very Dissatisfied\n2 = Dissatisfied  \n3 = Neutral\n4 = Satisfied\n5 = Very Satisfied\n\nYour feedback helps us improve our service!\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test 2 - Product Search Expected Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Final stage: satisfaction_survey\nRecommendations: 2 products\nResponse: Thank you for contacting TechMart support! Before we end this conversation, \ncould you please rate your experience today on a scale of 1-5?\n\nYour feedback helps us improve our service!\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test 3 - Billing Escalation Expected Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Final stage: escalated_complete\nEscalated: True\nTicket ID: BIL-2024-5678\nResponse: I've created a priority support ticket for you: BIL-2024-5678\n\nA human specialist will review your case and contact you within 2 hours at bob@example.com.\n\nIs there anything else I can help you with while you wait?\n"})}),"\n",(0,r.jsx)(n.h2,{id:"pro-tips-and-tricks",children:"Pro Tips and Tricks"}),"\n",(0,r.jsxs)(n.h3,{id:"1-state-management-best-practices",children:["1. ",(0,r.jsx)(n.strong,{children:"State Management Best Practices"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Good: Use typed state with defaults\nclass robustState(TypedDict):\n    messages: Annotated[List, add_messages]\n    retry_count: int = 0  # Default value\n    error_log: List[str] = []\n    \n# \u2705 Good: Validate state in nodes\ndef safe_node(state: CustomerServiceState):\n    if not state.get("messages"):\n        return {"messages": [("system", "No messages found")]}\n    \n    # Continue with processing...\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"2-performance-optimization-tricks",children:["2. ",(0,r.jsx)(n.strong,{children:"Performance Optimization Tricks"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Use caching for expensive operations\nfrom functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef expensive_product_lookup(query: str):\n    # Cached database lookup\n    pass\n\n# \u2705 Implement timeouts\nimport asyncio\nfrom asyncio import timeout\n\nasync def llm_with_timeout(prompt: str, timeout_seconds: int = 30):\n    try:\n        async with timeout(timeout_seconds):\n            return await llm.ainvoke(prompt)\n    except asyncio.TimeoutError:\n        return "Request timed out"\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"3-error-handling-patterns",children:["3. ",(0,r.jsx)(n.strong,{children:"Error Handling Patterns"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def resilient_agent(state: CustomerServiceState):\n    """Agent with comprehensive error handling"""\n    try:\n        # Main logic\n        result = process_request(state)\n        return {"result": result, "error": None}\n    \n    except Exception as e:\n        retry_count = state.get("retry_count", 0)\n        \n        if retry_count < 3:\n            # Retry with exponential backoff\n            time.sleep(2 ** retry_count)\n            return {\n                "retry_count": retry_count + 1,\n                "error": f"Retry {retry_count + 1}: {str(e)}"\n            }\n        else:\n            # Escalate after max retries\n            return {\n                "needs_human_escalation": True,\n                "error": f"Max retries exceeded: {str(e)}"\n            }\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"4-advanced-routing-strategies",children:["4. ",(0,r.jsx)(n.strong,{children:"Advanced Routing Strategies"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Multi-condition routing\ndef smart_router(state: CustomerServiceState):\n    """Route based on multiple factors"""\n    user_message = state["messages"][-1].content.lower()\n    customer_tier = state.get("customer_tier", "standard")\n    \n    # VIP customers get priority routing\n    if customer_tier == "vip":\n        return "vip_agent"\n    \n    # Sentiment-based routing\n    if any(word in user_message for word in ["angry", "frustrated", "disappointed"]):\n        return "escalation_specialist"\n    \n    # Default routing\n    return route_by_intent(state)\n\n# \u2705 Load balancing between agents\ndef load_balanced_routing(state: CustomerServiceState):\n    """Distribute load across multiple agents"""\n    agent_loads = {\n        "agent_1": get_current_load("agent_1"),\n        "agent_2": get_current_load("agent_2"),\n        "agent_3": get_current_load("agent_3")\n    }\n    \n    # Route to least loaded agent\n    return min(agent_loads.items(), key=lambda x: x[1])[0]\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"5-testing-and-debugging-strategies",children:["5. ",(0,r.jsx)(n.strong,{children:"Testing and Debugging Strategies"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Unit test individual nodes\ndef test_intent_classifier():\n    test_state = {\n        "messages": [("user", "Where is my order?")],\n        "tools_used": []\n    }\n    \n    result = intent_classifier(test_state)\n    assert result["intent"] == "order_inquiry"\n    assert "intent_classifier" in result["tools_used"]\n\n# \u2705 Integration test full workflows\ndef test_complete_workflow():\n    """Test end-to-end workflow"""\n    test_cases = [\n        ("Where is my order?", "order_inquiry"),\n        ("I need a new phone", "product_search"),\n        ("Billing issue", "billing_issue")\n    ]\n    \n    for message, expected_intent in test_cases:\n        result = app.invoke({\n            "messages": [("user", message)],\n            "session_id": f"test_{expected_intent}",\n            "tools_used": []\n        })\n        \n        assert expected_intent in result["tools_used"]\n\n# \u2705 Performance monitoring\ndef monitor_performance(state: CustomerServiceState):\n    """Track performance metrics"""\n    metrics = {\n        "total_response_time": state.get("response_time", 0),\n        "tools_used_count": len(state.get("tools_used", [])),\n        "escalation_rate": 1 if state.get("needs_human_escalation") else 0,\n        "conversation_length": len(state.get("messages", []))\n    }\n    \n    # Log metrics to monitoring system\n    log_metrics(metrics)\n    \n    return state\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"6-memory-and-persistence-tricks",children:["6. ",(0,r.jsx)(n.strong,{children:"Memory and Persistence Tricks"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Use checkpointing for conversation persistence\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\npersistent_app = workflow.compile(checkpointer=memory)\n\n# \u2705 Conversation continuity\ndef continue_conversation(session_id: str, new_message: str):\n    """Continue existing conversation"""\n    config = {"configurable": {"thread_id": session_id}}\n    \n    result = persistent_app.invoke(\n        {"messages": [("user", new_message)]},\n        config=config\n    )\n    \n    return result\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"7-production-deployment-best-practices",children:["7. ",(0,r.jsx)(n.strong,{children:"Production Deployment Best Practices"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Environment-specific configurations\nimport os\nfrom enum import Enum\n\nclass Environment(Enum):\n    DEV = "development"\n    STAGING = "staging"\n    PROD = "production"\n\ndef get_llm_config(env: Environment):\n    """Get environment-specific LLM configuration"""\n    if env == Environment.PROD:\n        return {\n            "model": "gpt-4",\n            "temperature": 0.3,\n            "max_tokens": 1000,\n            "timeout": 30\n        }\n    else:\n        return {\n            "model": "gpt-3.5-turbo",\n            "temperature": 0.7,\n            "max_tokens": 500,\n            "timeout": 15\n        }\n\n# \u2705 Health checks and monitoring\ndef health_check():\n    """System health check"""\n    try:\n        # Test basic functionality\n        test_result = app.invoke({\n            "messages": [("user", "test")],\n            "session_id": "health_check",\n            "tools_used": []\n        })\n        \n        return {\n            "status": "healthy",\n            "response_time": test_result.get("response_time", 0),\n            "timestamp": datetime.now().isoformat()\n        }\n    except Exception as e:\n        return {\n            "status": "unhealthy",\n            "error": str(e),\n            "timestamp": datetime.now().isoformat()\n        }\n'})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-features-in-action",children:"Advanced Features in Action"}),"\n",(0,r.jsx)(n.h3,{id:"streaming-results-for-real-time-updates",children:"Streaming Results for Real-time Updates"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Stream results for better user experience\nasync def stream_customer_service(user_message: str, session_id: str):\n    """Stream responses in real-time"""\n    config = {"configurable": {"thread_id": session_id}}\n    \n    async for chunk in app.astream(\n        {"messages": [("user", user_message)]},\n        config=config\n    ):\n        # Process each chunk as it arrives\n        yield chunk\n\n# Usage example:\nasync def handle_streaming_request():\n    async for chunk in stream_customer_service("Where is my order?", "user123"):\n        print(f"Received: {chunk}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"dynamic-graph-modification",children:"Dynamic Graph Modification"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# \u2705 Modify graph behavior at runtime\ndef add_seasonal_agent(workflow: StateGraph):\n    """Add seasonal promotions agent during holidays"""\n    def seasonal_promotions_agent(state: CustomerServiceState):\n        # Holiday-specific promotions\n        return {\n            "messages": [("assistant", "Check out our holiday deals!")],\n            "tools_used": state["tools_used"] + ["seasonal_promotions"]\n        }\n    \n    workflow.add_node("seasonal_promotions", seasonal_promotions_agent)\n    \n    # Modify routing to include seasonal agent\n    def holiday_router(state: CustomerServiceState):\n        if is_holiday_season():\n            return "seasonal_promotions"\n        else:\n            return route_by_intent(state)\n    \n    return workflow\n'})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);